ニューラルネットワークの学習
  機械が学習していく過程を学習
  データ駆動
    機械学習やディープラーニングというのは、人のアイディアが介在しない状態で機械が学習していく

    特徴量・・・入力データから本質的なデータを的確に抽出できるように変換できる変換器
              主にベクトルがメイン
    機械学習はその特徴量を自分でみつけ、SVMやKNNにぶち込むことをいう
    ディープラーニングはその特徴量までも見つけ出し、学習していくのである。
    「end-to-end machine learning」とも呼ばれているのはこのような理由

  訓練データ(教師データ)とテストデータ
    学習データとしてふたつデータ
      訓練データ
      テストデータ
    があり、訓練データで機械を学習させテストデータで学習させた機械を回し精度や速さを確認する

    汎化能力・・・まだ見ぬデータ(訓練データに含まれないデータ)に対しての能力

    この汎化能力を獲得することこそが機械学習の目標

    この汎用性がない機械学習スタイルー＞過学習
    過学習
      あるデータセットだけに過度に対応してしまう状態


  損失関数
    ニューラルネットワークの学習でも、ある「１つの指標」によって現在の状態を表す。
    そしてその指標を基準として、最適な重みパラメータの探索を行うのです。
    指標こそが　ー＞　損失関数
      あくまで損失関数はニューラルネットワークの性能の悪さを示す

    一般的には、二乗和誤差や交差エントロピー誤差などが用いられる。

     二乗和誤差
      損失関数で最も有名
      E=1/2Σn=k(yk-tk)^2
      yk ニューラルネットワーク出力
      tk 教師データ
      k データの次元
      この関数はone-ho表記法を使っている。

      実装：
        def mean_squeared_error(y,t):
          return 0.5*np.sum((y-t)**2)

      この関数の出力値が小さいほど教師データに適合している。

    交差エントロピー
      これもよく用いられる
      E=-Σn=k(tk*logyk)
      yk ニューラルネットワークの出力
      tk 正解ラベル
      one-hot表現を使用

      def cross_entropy_error(y,t):
        delta = 1e-7
        return -np.sum(t*np.log(y+delta))

      deltaはここでは、np.log(0)のような計算に備えて
      極小な値を入れることでinfにならないようにしている

      訓練データを使って学習する
      ー＞訓練データに対する損失関数を求めその値をできるだけ小さくするような
        パラメータを探し出す。
        つまり訓練データが１００個あればその１００個の損失関数の和を指標とする。

  ミニバッチ学習
    訓練データ全ての損失関数の和を求めたいとすると、交差エントロピー誤差で立式すると
      E=-1/NΣn=NΣk=n(tnk*logynk)
      データ　N個
      tnk m個目のデータのk番目の値 教師データ
      ynk ニューラルネットワークの出力
      これにより、Nで割ることにより、１個あたりの「平均の損失関数」を求めている。
      平均化すると、訓練データの数に関係なくいつでも統一した指標が得られる。

    巨大なデータセットをこのプログラムで学習させたいとき、
    その膨大な訓練データから無作為にある枚数だけ選び出し(ミニバッチという)
    そのミニバッチごとに学習を行う。
    このような学習手法をミニバッチ学習という。

    プログラムに書き込む際にload_minstで
    np.random.choice(60000,10)で無作為に値を手に入れることができる。

    [バッチ対応版]交差エントロピー誤差の実装

    def cross_entropy_error(y,t):
      #y　ニューラルネットワークの出力 t　教師データ
      if y.ndim==1:
        t=t.reshape(1,t.size)
        y=y.reshape(1,y_size)

        batch_size=y.shape[0]
        return -np.sum(t*np.log(y))/batch_size

    one-hot表現ではない場合
    def cross_entropy_error(y,t):
      if y.ndim==1:
        t=t.reshape(1,t.size)
        y=y.reshape(1,y_size)

        batch_size=y.shape[0]
        return -np.sum(np.log(y[np.arrange(batch_size),t]))/batch_size
        #各データの正解ラベルに対応するニューラルネットワークの出力を抽出

        どうして、損失関数を設定するのか？
          例
          数字認識の場合
          ー＞認識精度が高くなるようなパラメータを獲得したいので、
          　　損失関数自体が二度手間ではないか？
          　　つまり、目標とすることはできるだけ認識精度が高くなるニューラルネットワークを獲得
            　することなので、「認識精度」を指標にすべきではないか？
            ans
            ->微分の役割に注目するとわかる。
            　認識精度を指標にしてはいけない理由は微分がほとんどの場所で０になるから

          認識精度を指標にした場合、パラメータが少しの値変化しても認識精度は何ら変更がない
          もし、認識精度は改善されても、その値は連続的なものではなく、グラフ（ステップ関数）
          のような不連続の飛び飛びの値へと変わる。
          ステップ関数とシグモイド関数において、ニューラルネットワークの活性化関数として行えない
          理由として微分したときほとんど０になるからシグモイド関数が主に活性化関数。

          微分
            ある瞬間の変化の量を表したもの
            悪い実装例
              def numerical_diff(f,x):
                h = 10e-50 #0.0000...01(0が５０個)
                return (f(x+h)-f(x))/h

              numerical defferentiation => 数値微分
              上の関数は２つ悪いところがある。
               h=10e-50が丸め誤差問題になっている。
                丸め誤差とは、小数の小さな範囲において、数値が省略されることで最終的な計算
                結果に誤差が生じること。
               関数fの差分
               　前方差分ではなく(x+h)と(x-h)の中心差分に変更
               　　理由：xとx＋hだと厳密には真の微分値のxから多少なりともずれるから。
              以上の2点より
              def numerical_diff(f,x):
                h = 1e-4 #0.0001
                return (f(x+h)-f(x-h))/(2*h)

              ちなみに
                数式の展開によって微分を求めることは解析的という言葉を用いる
                例　y=x^2の微分は解析的にdy/dx=2*xとしてとくことができる

          数値微分の例
            y=0.01*x^2+0.1*x
            上の文をコードで
            def function_1(x):
              return 0.01*x**2+0.1*x

            例：./func1.py
            import numpy as np
            import matplotlib.pylab as plt
            def function_1(x):
                return 0.01*x**2+0.1*x
            def numerical_diff(f,x):
                h = 1e-4 #0.0001
                return (f(x+h)-f(x-h))/(2*h)

            x=np.arange(0.0,20.0,0.1)# 0~20 0.1刻みのx配列
            y=function_1(x)
            plt.xlabel("x")
            plt.ylabel("f(x)")
            plt.plot(x,y)
            print(numerical_diff(function_1,5))
            #0.1999999999990898　厳密に言えば0.2
            print(numerical_diff(function_1,10))
            #0.2999999999986347　厳密に言えば0.3
            plt.show()

            具体的にグラフでその微分した計算式を表現したい場合は
            ../3rd/deep-learning-from-scratch/ch04/gradent_1d.py
            を実行
        偏微分
          １変数の微分と同じで、ある場所の傾きを求めます。ただし、
          複数ある変数のなかでターゲットとする変数を１つに絞り他の変数は
          ある値に固定します。

        勾配 gradient
          すべての変数の偏微分をベクトルとしてまとめたもの
          def numerical_gradient(y,x):
            h=1e-4
            grad=np.zeros_like(x) #xと同じ形状の配列を生成して０を入れる

            for idx in range(x.size):
              tmp_val=x[idx]
              #f(x+h)
              x[idx]=tmp_val+h
              fxh1 =f(x)
              #f(x-h)
              x[idx]=tmp_val-h
              fxh2 =f(x)

              grad[idx]=(fxh1-fxh2)/(2*h)
              x[idx]=tmp_val #値を元に戻す

            return tmp_val

            上の勾配のことを理解するために、ch03/gradient_2d.pyを実行
              関数f(x0,x1)の「１番低い場所」をさしていて矢印の群は一点を向いている１番
              遠く離れれば離れるほど矢印の大きさも大きくなる
              確に言うならば、勾配が示す方向は
                                  各場所において関数の値を最も減らす方向

        勾配法
          損失関数は複雑
          パラメータ空間は広大であり、どこに最小値をとる場所があるのか
          見当がつかない
          ー＞勾配をうまく使って関数の最小値を探そう
            　うまく使う＝勾配方向へ進むことを繰り返すことで関数の値を徐々に減らすのが勾配法
          勾配が０
          ー＞最小値だけじゃなく、極小値や鞍点(saddle point)
            鞍点とはある方向では極大値、違う方向では極小値みたいな点

          勾配法を公式に
            x0=x0 - η*(∂f/∂x0)
            x1=x1 - η*(∂f/∂x1)
            ηは更新の量、ニューラルネットワークの学習において、学習率という
              学習率、どれだけパラメータを更新するべきか
              ニューラルネットワークの学習においては
              学習率の値を変更しながら正しく学習できているか確認作業を行うことが一般的

            勾配法の実装
            def gradient_descent(f,init_x,lr=0.01,step_num=100):
              x=init_x
              for i in range(step_num):
                grad=numerical_diff(f,x)
                x -= lr*grad
              return x

            f 最適化したい関数
            init_x 初期値
            lr learning rate 学習率
            step_num　勾配法による繰り返しの数
            ch04/gradient_method.py で勾配法を実装してy=x0^2+x1^2の結果がでる
            ソースコードがあるから試してみなさい

            学習率が大きすぎると大きな値へと発散してしまう
            学習率が低すぎるとほとんど更新されずに終わってしまう。
            適切な学習率を設定すると言うことが重要

            学習率−＞ハイパーパラメータ
                    こいつは人の手によって設定されるパラメータ
                    一般的にはこのハイパーパラメータを色々な値で試しながらうまく学習できる
                    ケースを探すという作業が必要

        ニューラルネットワークに対する勾配
          重みパラメータに関する損失関数の勾配
          ch04/gradient_simplenet.py
          2x3の形状の勾配の実装
          import sys, os
          sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
          import numpy as np
          from common.functions import softmax, cross_entropy_error
          from common.gradient import numerical_gradient


          class simpleNet:
            def __init__(self):
                self.W = np.random.randn(2,3) #ガウス分布(２x３)で初期化
            def predict(self, x):#予測するためのメソッド
                return np.dot(x, self.W)
            def loss(self, x, t):#損失関数をもとめるためのメソッド
                #x 入力データ　t 正解ラベル
                z = self.predict(x)
                y = softmax(z)
                loss = cross_entropy_error(y, t)　#交差エントロピー誤差
                return loss

          x = np.array([0.6, 0.9])
          t = np.array([0, 0, 1])

          net = simpleNet()
          #この１文が勾配を求めている
          f = lambda w: net.loss(x, t)
          ###
          lamdaで表さないと下のようになる
          def f(W):
          #Wはダミーでつけたもの。これは、numerical_gradient(f,x)が内部でf(x)を実行するため、それと整合性がとれるように
            return net.loss(x,t)
          ###
          dW = numerical_gradient(f, net.W)
          #ここで
          print(net.W)
          #例のランダムで与えられるパラメータ
          [[-0.0019881   0.42256403  0.07264316]
           [-0.1064055   0.51516922  0.0729388 ]]
          print(dW)
          #上の値を損失関数で勾配したパラメータ
          ∂L/∂W =[[ 0.13374235  0.30189031 -0.43563266]
                  [ 0.20061353  0.45283546 -0.65344898]]

          勾配したデータである、パラメータで∂L/∂W11は0.13ですが、
          これはw11をhだけ増やすと損失関数の値は0.13hだけ増加するということになる
          またw23はおよそ-0.6ですが、これはw23をhだけ増やすと損失関数の値は損失関数の値は-0.6hだけ減少する
          そのため、損失関数を減らすという観点からは
            w23はプラス方向へ更新し、
            w11はマイナス方向へ更新するのが
          良いことになります。
          また、更新の度合いについても、w23のほうがw11よりも大きく貢献するということがわかります。

          ニューラルネットワークの勾配を求めれば、あとは、勾配法に従って、重みパラメータを更新するだけです。
