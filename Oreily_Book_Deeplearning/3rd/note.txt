ニューラルネットワークとは　
  主に三層で分けられていて、入力層、中間層(隱れ層)、出力層と分けられている
  それぞれの層でノードを持っている

パーセプトロンとは
      {0 b+w1x1+w2x2 <=0}
    y={1 b+w1x1+w2x2 > 0}
    b=バイアスとは閾値である。

  上のものを
    y=h(b+w1x1+w2x2)とし
         {0 x<=0}
    h(x)={1 x> 0}
    と変換
    このように入力信号の総和を出力する関数を活性化関数と呼ぶ
    活性化関数は入力信号の総和がどのように活性化するか(どのように発火するか)ということを
    決定する役割

    これをまた書き換えると
    上の式は重み付きの入力信号の総和を計算しその話が活性化関数によって変更されている

    a=b+w1x1+w2x2
    y=h(a)

    1  b      h()
    x1 w1 > a -> y
    x2 w2

    上のような形である
    活性化関数は、パーセプトロンからニューラルネットワークにつながる架け橋である

活性化関数
  ステップ関数、階段関数・・・閾値を境にして出力が切り替わる関数
  パーセプトロンでは、活性関数にステップ関数を利用している
  パーセプトロンではステップ関数を利用している
  ニューラルネットワークではそのステップ関数を変更することで入れる

シグモイド関数
  ニューラルネットワークでよく用いられる活性関数の１つ

  h(x)=1/1+exp(-x)
  exp(-x)=e^-x

ステップ関数の実装
def step_function(x):
    if x>0:
        return 1
    else:
        return 0
これはいたって簡単だが、float型しか受け取れなく、行列がだめである。
だからnumpyをつかって

import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x > 0,dtype=np.int)
    #dtypeとは返り値の型であり、ただ、x>0だけだとboolean型が返り値だから

x=np.arange(-5.0,5.0,0.1)
#-5.0から5.0までの範囲を0.1刻みでNumPy配列を生成します。
y=step_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)#y軸の範囲の指定
plt.show()

シグモイド関数の実装
import numpy as np
import matplotlib.pylab as plt

def sigmoid_function(x):
    return 1/(1+np.exp(-x))

x=np.arange(-5.0,5.0,0.1)
y=sigmoid_function(x)
plt.plot(x,y)
plt.ylim(-0.1,1.1)#y軸の範囲の指定
plt.show()

このときシグモイド関数ではNumpyのブロードキャストを利用して、
配列でも計算できるようにしている

シグモイド関数とステップ関数との相違
  図をみてみると、２つは滑らかさが違う
  シグモイド関数は入力に対して、連続的な出力を出している。
  それ自体がニューラルネットワークの学習において重要なのである。
２つの共通部分
  入力信号が重要な情報であればあるほど、大きな値を出力
  逆もまたしかり
  非線形関数
    線形関数・・・出力が入力の程倍数であること(y=ax)

  活性化関数が線形関数ではない理由
    層を重ねることの意味がなくなってしまうから。
    (例えばh(x)=cx(活性化関数)とする。そのとき、y(x)=h(h(h(x)))
    を行う三層のネットワークを対応させようとする。そのときy(x)=c*c*c*xとなるがこれは
    y(x)=a*x

########################################################################
    import numpy as np

    def sigmoid(x):
        #sigmoid関数の定義
        #これも入力層と中間層の活性化関数として、利用されている。
        return 1/(1+np.exp(-x))


    def init_network():
        #重みとバイアスの初期化
        network={}
        network['W1']=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])
        network['b1']=np.array([0.1,0.2,0.3])
        network['W2']=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
        network['b2']=np.array([0.1,0.2])
        network['W3']=np.array([[0.1,0.3],[0.2,0.4]])
        network['b3']=np.array([0.1,0.2])

        return network

    def identity_function(x):
        #出力層の活性化関数として利用される
        #恒等関数。。。入力をそのまま出力する関数
        return x

    def forward(network,x):
        #入力信号が出力へと変換されるプロセスはまとめて実装
        W1,W2,W3=network['W1'],network['W2'],network['W3']
        b1,b2,b3=network['b1'],network['b2'],network['b3']

        a1=np.dot(x,W1)+b1# 信号伝達
        z1=sigmoid(a1)#h(x)
        a2=np.dot(z1,W2)+b2
        z2=sigmoid(a2)
        a3=np.dot(z2,W3)+b3
        y=identity_function(a3)#σ(x)

        return y

    network=init_network()
    x=np.array([1.0,0.5])#x１x２を準備
    y=forward(network,x)#ニューラルネットワークの実装
    print(y)
########################################################################

実装完了
このとき、出力での活性化関数の恒等関数は、必要に応じて変わる。
回帰問題では恒等関数
２クラス分類問題ー＞シグモイド関数
多クラス分類問題ー＞ソフトマックス関数
使うのが一般的

出力層の設計
  分類問題・・・データがどのクラスに属するか
    人の映った画像から、その人が男か女かのどちらかを分類するような問題
  回帰問題・・・ある入力データから、(連続的な)数値の予測を行う問題
    人の映った画像から、その人の体重を予測するような問題

  恒等関数・・・入力をそのまま出力にする。
  分類問題で使われるソフトマックス関数
    yk=exp(ak)/nΣi=1exp(ai)
    入力信号 ak 全ての入力信号の和
    np.exp(a)・・・aの指数関数
    np.sum(a)・・・aの関数の和
    ソフトマックス関数は計算式通りだからpass
  ソフトマックス関数の実装上の注意
    上の通り進むとダメ
      理由として、オーバーフローつまり、指数鑵子の計算で、値が容易に大きな値になるからe^1000とかだと
      inf(無限)を返す。そうすると不安定な結果になる。
      だから
        yk=exp(ak)/nΣi=1exp(ai)
          =Cexp(ak)/CnΣi=1exp(ai)
          =exp(ak+logC)/nΣi=1exp(ai+logC)
          =exp(ak+C')/nΣi=1exp(ai+C')(C'=-(-C'))
          =exp(ak-(-C'))+nΣi=1exp(ai-(-C'))
      これより、ソフトマックスの指数関数のけいさんを」行う際に何らかの足し算(引き算)をしても
      結果は変わらないことがわかる。

      ソフトマックスの総和は１で、これは重要な性質
        問題に対して確率的な（統計的）な対応ができるから
      注意
        各要素の大小関係は変わらないから。
        また、ニューラルネットワークのクラス分類では、一般的に出力の１番大きいニューロンに
        相当するクラスだけ認識結果とする。だからソフトマックス関数を適用しても、出力の一番
        大きいニューロンの場所は変わりません。だから、ニューラルネットワークが分類を行う際には
        出力層のソフトマックス関数を省略することができる。

      出力層のニューロン層の数
        この数は、とくべき問題におうじて、適宣決める必要がある。
        クラス分類を行う問題では、出力層のニューロンの数は分類したいクラスの数に設定するのが、
        一般的。
